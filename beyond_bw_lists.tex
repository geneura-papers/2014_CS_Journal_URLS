\documentclass[preprint]{elsarticle}
\biboptions{round, numbers}
\usepackage[latin1]{inputenc}
%\usepackage[T1]{fontenc}
%\usepackage{textcomp}
\usepackage{graphicx}
%\usepackage{color}
%\usepackage{setspace}
\usepackage{url}
\usepackage[english]{babel}
\usepackage{todonotes}

\begin{document}

\begin{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   TITLE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{[TENTATIVE] Can I navigate this web? Controlling URL accesses in the enterprise by means of categorical classifiers}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   AUTHORS   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{P. de las Cuevas, A.M. Mora, J.J. Merelo}
\ead{\{paloma, amorag, jmerelo\}@geneura.ugr.es}
\address{Departamento de Arquitectura y Tecnología de Computadores.\\ ETSIIT - CITIC. University of Granada, Spain}
%\author{A. M. Mora}
%\ead{amorag@geneura.ugr.es}
%\address{Departamento de Arquitectura y Tecnología de Computadores. Escuela Técnica Superior de Ingenierías Informática y de Telecomunicación. CITIC. University of Granada, Spain}

%\maketitle

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   ABSTRACT   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{abstract} 

\end{abstract}

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   KEYWORDS   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{keyword}
Black List \sep White Lists \sep Data Mining \sep Corporate Security Policies \sep URL request\sep Machine Learning \sep Classification.
\end{keyword}

\end{frontmatter}


%-------------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   INTRODUCTION   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-------------------------------------------------------------------------------

\section{Introduction}
\label{sec:intro}

The concept of Security inside an Enterprise can be addressed from the point of view of the Internet connections that are daily being made. The employees who make these might have working purposes or not. The Enterprise, which is aware of this situation, wants to reduce the risks of security attacks introduced by some non trustworthy websites. Then, on a second level, but still important, companies want their employees to not to have their productivity decreased.

Above all the security tools that enterprises normally use, in this paper we want to focus on those named as Blacklists and Whitelists. Their use is widely extended, as they are easy to maintain, and each one has its advantages and disadvantages. The use of both highly increases the security in a company.
Indeed it is true that both Blacklists and Whitelists are easy to update or to maintain, because every day, new bad sites are reported. But at the same time, new sites (dangerous or not) are created. Netcraft reports from November \cite{netcraft:site} show that there are about 950 million active websites. But McAfee reported \cite{mcafee:site} that, at the end of the first quarter of 2014, there were more than 18 million new suspect URLs (2 million associated domains), and also more than 250 thousand new pishing URL (almost 150 thousand associated domains).

With this scenario, companies need to be able to update their Blacklists and Whitelists not only by what is reported by security servers but also by learning from the connections that the employees made in the past and were known as dangerous. This way, anomalous situations can be detected and also classified as candidates for the Blacklist or the Whitelist.

This work shows that this can be achieved by using categorical classifiers. It is possible to gather information from the log files that the companies proxies store, and also from the set of Company Security Policies (CSPs), in order to be able of label the entries of the log and obtain a data set. That data set has been the one used for training and testing a set of chosen classifiers, being all of them capable of support categorical data given the nature of the information in a URL connection log file. We will show how the classifiers have obtained rules, for classifying unknown URLs, that are built with more antecedents than the URL only, as normal Black and White lists do.

The paper is structured as follows...

%----------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%   BACKGROUND AND RELATED WORK %%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------------------

\section{Background and related work}
\label{sec:background_sota}

%*** Esta sección presentará conceptos de seguridad en la empresa, así como conceptos de machine learning y sus aplicaciones dentro de la seguridad corporativa, en concreto en aproximaciones similares a esta
%Creo que no tiene que ver y es repetir lo mismo all over again

*** Hay que diferenciar este trabajo de los existentes y destacar su avance en el estado del arte

%Network and internet security in enterprises


%Data

The datasets that have been traditionally used for this kind of studies are the original KDD 99 Cup Dataset \cite{kddcup99}, and its improved version NSL KDD \cite{nslkdd}. Both are detailed by Tavallaee, Mahbod et al. \cite{tavallaee2009detailed}, and the first constists of a group of connection logs from a normal network, with simulated attack traffic added. The second version, which may be called as improved or refined, had the same log of connections from the initial dataset, but having removed the redundant traffic. This means that there might have been duplicated records in the original dataset that could mislead the results after the training and testing processes.

In our work we make a similar approach. As will be deeply explained in Section \ref{sec:metodology}, we also removed the duplicated entries from the connections log. However, the main advantage of the data we have used with respect to the others used in previous works, is that we obtained the log directly from an actual company, thus we are working with more accurate datasets than those which include simulated traffic.

%URL classification

Also, in this work we are dedicating an important part to URL, its different parts and the way they influence in the classification process. We have reviewed some works related with the study of URLs. In particular, we found interesting those works that try to identify malicious sites (like the ones that want to perform a pishing attack). It is also interesting if they study the URL lexical features, like the one performed during this Master Thesis, and because we consider this kind of study better than to download and proccess the page (the thing that in fact is trying to be avoided). 

Hence, Kan and Thi \cite{Kan_URL05} focus their work in lexical features in order to classify as dangerous, URLs that were not previously in Blacklists servers. They gather features like the URI components, length, ortographic data, or segments by entropy reduction. Their results are close to 95\% of accuracy.

On the other hand, this work not only focuses in lexical features of the requested URL, but also in other data that appears in the log files. And not exactly log data but Zhang et al. \cite{Zhang_cantina07}, with CANTINA, detect pishing URLs by studying lexical features, content related features, and a WHOIS query (obtaining the date when the domain were registered, which if it is too new, it can be less trustful). They also obtain a 95\% of accuracy.

The most important work we have found related to this were of J. Ma et al \cite{Ma_Url11}, whose aim is to detect malicious URLs, mainly related with pishing attacks throug e-mailing, but without processing the content or other private data of the user. They extract information from the lexical features (62\% of the total of the gathered features), and also from the host that has the URL. It is important to point out that they perform the study over 100 days, and that they work with a quantity of almost 2 million of features. In addition, they implement an online classifier (instead of a batch one), and obtain a 99\% of accuracy. 

%Crime data mining and forensics in enterprise (?)

On the one hand, DM helped to develop new solutions to computer forensics \cite{DeVel2001}, being the researchers able to extract information from large files with events gathered from infected computers. Another important advance took place after the 9/11 events, when \textit{clustering techniques} and \textit{social network analysis} started to be performed in order to detect pontential crime networks \cite{Hsinchun2003}.
On the other hand, and more focused on the user side like our approach, there exist some user-centric solutions to problems like user authentication in a personal device, who Greenstadt and Beal \cite{cognitive_security_08} proposed to address using collected user biometrics along with machine learning techniques.

%feature extraction

%---------------------------------------------------------------------
%\subsection{Corporate Security}
%\label{subsec:corporatesecurity}

%---------------------------------------------------------------------
%\subsection{Machine Learning}
%\label{subsec:dm+ml}



%-------------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  PROBLEM DEFINITION  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-------------------------------------------------------------------------------

\section{Problem definition}
\label{sec:problem_data}

%*** Definir el problema a resolver y describir los datos que se van a manejar en el mismo.
Being introduced in Section \ref{sec:intro}, the problem this work addresses is the ability of self-adaptation that company network security systems have, for many new dangerous websites emerge every day. Corporate Security Policies (CSPs) may include Black and White lists to cope with dangerous situations, but always for known and already classified websites. Therefore, our goal is to demonstrate that is possible to automatically classify, as allowed or denied, a URL that was not in a Blacklist or a Whitelist. Also, we want that the final application of an `ALLOW' or `DENY' label depends on other features of that URL (lexical, or contextual, for instance), going then a step beyond the Black and White lists.

The data used to demonstrate this has been provided by an actual Spanish company and it has been anonymized. Since our purpose is to have a dataset with a set of connections of which it is already known its permission, we have used two different files:

\begin{itemize}
   \item A log file with the connections made by employees of the company. % ¿El de 2 horas o el de 1 día?
   This log contains the typical information of an HTTP request, it is gathered by the company Squid proxy \cite{squid:site}, and its format is CSV. 
   \item A set of rules wich decide, mostly depending on the URL core domain, if a connection should be allowed (Whitelist) or denied (Blacklist). The rules have been provided by the same company.
 \end{itemize} 

In order to have enough information to train the classifiers, we have taken as features the very same fields that the Squid log provides, being those of an HTTP request: the HTTP reply code and method, the time when the connection was made and how much did it take to the server to answer, the IP addresses of the client and requested server, the content type of the webpage and its size in bytes, and the complete URL string.
At the same time, the URL has the following structure:

\begin{verbatim}
http://www.subdomain5.....subdomain1.url_core.url_TLD/folder1/.../
/filename.filename_extension
\end{verbatim}

Thus a lot of features can be obtained from the URL string.
It is important to point out that, from all the information which can be gathered from each entry of the log, not every field (or feature) is dependant on the user. Table \ref{featuretype} shows the set of initial features extracted from the log of connections. For each feature, its type is indicated and also the dependency with the user. This first study over the features can help to the feature selection, because what matters for identifying non desirable users behaviour, would be the information that depends on the user.

\begin{table*}[htpb]
\centering
 \caption{\label{featuretype} Extracted features from each entry of the Log and its dependance or independance on the user behaviour.}
{\scriptsize
\begin{tabular}{|l|l|l|}
\hline
\textbf{Feature name} & \textbf{Feature type} & \textbf{Relationship with user behaviour}\\ 
\hline
\texttt{http\_reply\_code} & Categorical & Independent \\ 
\texttt{http\_method} & Categorical & Independent \\ 
\texttt{duration\_milliseconds} & Numeric & Independent \\
\texttt{content\_type\_MCT} & Categorical & Independent \\ 
\texttt{content\_type} & Categorical & Independent \\ 
\texttt{server\_or\_cache\_address} & Categorical & Independent \\
\texttt{time} & Date & Dependent \\ 
\texttt{squid\_hierarchy} & Categorical & Independent \\ 
\texttt{bytes} & Numeric & Independent \\  
\texttt{URL\_length} & Numeric & Dependent \\  
\texttt{letters\_in\_URL} & Numeric & Dependent \\  
\texttt{digits\_in\_URL} & Numeric & Dependent \\  
\texttt{nonalphanumeric\_chars\_in\_URL} & Numeric & Dependent \\  
\texttt{url\_is\_IP} & Boolean & Dependent \\  
\texttt{url\_has\_subdomains} & Boolean & Dependent \\  
\texttt{num\_subdomains} & Numeric & Dependent \\  
\texttt{subdomain5} & Categorical & Dependent \\  
\texttt{subdomain4} & Categorical & Dependent \\  
\texttt{subdomain3} & Categorical & Dependent \\  
\texttt{subdomain2} & Categorical & Dependent \\  
\texttt{subdomain1} & Categorical & Dependent \\  
\texttt{url\_core} & Categorical & Dependent \\  
\texttt{url\_TLD} & Categorical & Dependent \\  
\texttt{url\_has\_path} & Boolean & Dependent \\  
\texttt{folder1} & Categorical & Dependent \\  
\texttt{folder2} & Categorical & Dependent \\  
\texttt{path\_has\_parameters} & Boolean & Dependent \\  
\texttt{num\_parameters} & Numeric & Dependent \\  
\texttt{url\_has\_file\_extension} & Boolean & Dependent \\  
\texttt{filename\_length} & Numeric & Dependent \\  
\texttt{letters\_in\_filename} & Numeric & Dependent \\  
\texttt{digits\_in\_filename} & Numeric & Dependent \\  
\texttt{other\_char\_in\_filename} & Numeric & Dependent \\  
\texttt{file\_extension} & Categorical & Dependent \\  
\texttt{url\_protocol} & Categorical & Dependent \\ 
\texttt{client\_address} & Categorical & Dependent \\
\hline
\end{tabular}
}
\end{table*}

Then, every original rule covers a number of entries, and so a class is applied to them. This class is a `label' with two possible values: `allow', or `deny'. An `allow' class is applied to an entry when that entry suffices the conditions of a rule which permits the connection, or because the URL is included in the Whitelist. On the contrary, the `deny' class is assigned to those entries that are not permitted by the company, then, they will fit the rules that forbid those connections, or their URLs will be included in the Blacklist.

%----------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   METODOLOGY  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------------------

\section{Metodology}
\label{sec:metodology}

In order to have the extracted information in a proper way to train and test the classifiers, a preprocessing is performed over both log and rules files. From now on, we treat Black and White lists as set of rules too, given the fact that we can express every entry of those lists as:

\begin{verbatim}
IF
url = some_allowed_or_denied_url
THEN
allow/deny
\end{verbatim}

%*** Describir la metodología a seguir para la creación de los juegos de datos:
%- preprocesado
%- etiquetado para componer datos a clasificar
%- técnicas de balanceo

%*** Describir los 3 grandes bloques y justificarlos:
%- Datos iniciales
%- Características extraídas
%- Sesiones

All Java code for the realisation of this work is available at Github \cite{github:site}. % Short url http://git.io/4cQYFQ

%---------------------------------------------------------------------
\subsection{Initial Data}
\label{subsec:initial_data}

*** Describir los primeros datos y procesamientos (eliminar duplicados, agrupar redundantes, etc)


%---------------------------------------------------------------------
\subsection{Extracting Features}
\label{subsec:extracting_features}

*** Describir y justificar las características extraídas de los datos iniciales.

%---------------------------------------------------------------------
%\subsection{Grouping in Sessions}
%\label{subsec:sessions}

%*** Describir y justificar el proceso de agrupamiento en sesiones

% Esto al final no entra en este artículo



%----------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%   EXPERIMENTS AND RESULTS  %%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------------------

\section{Experiments and Results}
\label{sec:experiments}

*** Describir los experimentos realizados, en cada bloque.
*** Analizar los resultados obtenidos en cada bloque

%---------------------------------------------------------------------
\subsection{Initial Data Results}
\label{subsec:initial_data_results}

*** Experimentos y resultados sobre el conjunto original y los conjuntos 'refinados' (TLD, sin duplicados, etc)


%---------------------------------------------------------------------
\subsection{Extracted Features Results}
\label{subsec:extracted_features_results}

*** Experimentos y resultados sobre el conjunto con las nuevas características


%---------------------------------------------------------------------
\subsection{Sessions Results}
\label{subsec:sessions_results}

*** Experimentos y resultados sobre el conjunto agrupando por sesiones



%----------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   DISCUSSION  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------------------

\section{Discussion}
\label{sec:discussion}

*** Comentar los resultados de cada método y analizar las ventajas e inconvenientes de cada uno, así como las mejoras que se hayan conseguido con la extracción de características y el agrupamiento por sesiones


%----------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   CONCLUSIONS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------------------

\section{Conclusions and Future Work}
\label{sec:conclusions}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  ACKNOWLEDGEMENTS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Acknowledgements}
This work has been supported by the European project MUSES (FP7-318508).

\bibliographystyle{elsarticle-num}
\bibliography{review_muses,data_mining_urls,ci_security_rules}

\end{document}
